{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackGAN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiRoxRoVOX4a",
        "colab_type": "text"
      },
      "source": [
        "# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Stack-GAN**\n",
        "\n",
        "> The generator\n",
        "networks in both stages are Conditional Generative Adversarial\n",
        "Networks (CGANs). The first GAN is conditioned on the text\n",
        "descriptions, while the second network is conditioned on the text\n",
        "descriptions and the images generated by the first GAN as well\n",
        "& binary-cross-entropy uses as our loss function in both stages.\n",
        "\n",
        "***Powered by:***\n",
        "\n",
        "![uniXerr logo](https://drive.google.com/uc?id=1TXJwfJsTJzU2M7LrIQgx2Tx4cfUzcQuX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaAnm-tkwYEN",
        "colab_type": "text"
      },
      "source": [
        "![gds](https://drive.google.com/uc?id=1Q1hdN4xJfcCfZNnmKU3sGfi64C-dWETX)\n",
        "\n",
        "![sgan](https://drive.google.com/uc?id=1QkMK7mZFNeXsQXuszgXbowG_SQRgl2jj)\n",
        "\n",
        "The architecture of the proposed StackGAN. The Stage-I generator draws a low-resolution image by sketching rough shape and\n",
        "basic colors of the object from the given text and painting the background from a random noise vector. Conditioned on Stage-I results, the\n",
        "Stage-II generator corrects defects and adds compelling details into Stage-I results, yielding a more realistic high-resolution image.\n",
        "\n",
        "![sgan](https://drive.google.com/uc?id=1HMzOx6xnaWe2TtOACCVsLV4gjQMNebPR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se0a6ginxHlH",
        "colab_type": "text"
      },
      "source": [
        "*   `t` This is a text description of the true data distribution.\n",
        "*   `z` This is a randomly sampled noise vector from a Gaussian distribution.\n",
        "*   `φt` This is a text embedding of the given text description\n",
        "generated by a pre-trained encoder.\n",
        "*   `Ĉ0` This text conditioning variable is a Gaussian conditioning\n",
        "variable sampled from a distribution . It captures the\n",
        "different meanings of.\n",
        "*   `N(μ(φt), Σ(φt))` This is a conditioning Gaussian distribution.\n",
        "*   `N(0, I)` This is a normal distribution.\n",
        "*   `Σ(φt)` This is a diagonal covariance matrix.\n",
        "*   `pdata` This is the true data distribution.\n",
        "*   `Pz` This is the Gaussian distribution.\n",
        "*   `D1` This is the Stage-I discriminator.\n",
        "*   `G1` This is the Stage-I generator.\n",
        "*   `D2` This is the Stage-II discriminator.\n",
        "*   `G2` This is the Stage-II generator.\n",
        "*   `N2` These are the dimensions of the random noise variable.\n",
        "*   `Ĉ`  These are the Gaussian latent variables for the Stack-II GAN.\n",
        "\n",
        "![symbol](https://drive.google.com/uc?id=1SrLUphhN566ECg9arCzRP418Yz7bM6pb)\n",
        "\n",
        "![sgan-s1](https://drive.google.com/uc?id=1ahtUJcScvngZ-vpR_-uT6tRaL6AS6Kt5)\n",
        "\n",
        "![sgan-s2](https://drive.google.com/uc?id=1srovKaptPDvohKkjCwyzrzjCu_02Z_y-)\n",
        "\n",
        "![sgan-s2-c](https://drive.google.com/uc?id=1s5iK7yqcF_9onHqp_znrP5T8YSHjGtc5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExtXKYFPuYtb",
        "colab_type": "text"
      },
      "source": [
        "***StackGAN Papers & Codes:***\n",
        "\n",
        "[AttnGAN - code](https://github.com/taoxugit/AttnGAN)\n",
        "\n",
        "[StackGAN-v1 - paper](https://arxiv.org/pdf/1612.03242.pdf)\n",
        "\n",
        "[StackGAN-v2 - code](https://github.com/hanzhanggit/StackGAN-v2)\n",
        "\n",
        "[StackGAN-v1 - code](https://github.com/Vishal-V/StackGAN)\n",
        "\n",
        "[StackGAN-v1-Stage2 - code](https://github.com/mrrajatgarg/StackGAN-Keras-implementation/blob/master/stackgan_stage_2_implementation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic95ajPZzPYp",
        "colab_type": "text"
      },
      "source": [
        "**TODO**\n",
        "\n",
        "**1)** find the answers of questions. \n",
        "\n",
        "**2)** complete `TPARAM.plog__it` function. \n",
        "\n",
        "**3)** implement stage2 \n",
        "\n",
        "**4)** complete training & prediction parts "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd8NjI6DcyPS",
        "colab_type": "text"
      },
      "source": [
        "# **Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6vzKWsc4aU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44de317b-a247-4cde-9c9b-276eaf7a901d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QcizkWz1LWsU"
      },
      "source": [
        "# **Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YW10oNaqLWp8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "14d1b2c8-cae9-48e6-96ec-d9bfcb495e72"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from PIL import Image\n",
        "import pprint\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import os\n",
        "import sys\n",
        "import imageio\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import asyncio\n",
        "import math\n",
        "import seaborn as sns\n",
        "import tensorflow.keras.backend as K\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU, ReLU\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Multiply\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "\n",
        "# Confirm that we're using Python 3\n",
        "assert sys.version_info.major is 3, 'Switch to python3 please!'\n",
        "print(\"[...] Installing dependencies for Colab environment\\n\\n\")\n",
        "!pip install plotly==4.6.0\n",
        "!pip install -Uq grpcio==1.26.0\n",
        "!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca\n",
        "!chmod +x /usr/local/bin/orca\n",
        "!apt-get install xvfb libgtk2.0-0 libgconf-2-4\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning:\n",
            "\n",
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[...] Installing dependencies for Colab environment\n",
            "\n",
            "\n",
            "Requirement already satisfied: plotly==4.6.0 in /usr/local/lib/python3.6/dist-packages (4.6.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly==4.6.0) (1.12.0)\n",
            "--2020-04-30 16:31:50--  https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/99037241/9dc3a580-286a-11e9-8a21-4312b7c8a512?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200430T163023Z&X-Amz-Expires=300&X-Amz-Signature=ff74270f681d826ac80ea6e9d02a1a320fa79781119d5fa1e216bfef55d3793e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=99037241&response-content-disposition=attachment%3B%20filename%3Dorca-1.2.1-x86_64.AppImage&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-04-30 16:31:50--  https://github-production-release-asset-2e65be.s3.amazonaws.com/99037241/9dc3a580-286a-11e9-8a21-4312b7c8a512?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200430%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200430T163023Z&X-Amz-Expires=300&X-Amz-Signature=ff74270f681d826ac80ea6e9d02a1a320fa79781119d5fa1e216bfef55d3793e&X-Amz-SignedHeaders=host&actor_id=0&repo_id=99037241&response-content-disposition=attachment%3B%20filename%3Dorca-1.2.1-x86_64.AppImage&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.250.44\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.250.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51607939 (49M) [application/octet-stream]\n",
            "Saving to: ‘/usr/local/bin/orca’\n",
            "\n",
            "/usr/local/bin/orca 100%[===================>]  49.22M  66.4MB/s    in 0.7s    \n",
            "\n",
            "2020-04-30 16:31:51 (66.4 MB/s) - ‘/usr/local/bin/orca’ saved [51607939/51607939]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgtk2.0-0 is already the newest version (2.24.32-1ubuntu1).\n",
            "libgconf-2-4 is already the newest version (3.2.6-4ubuntu1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmYNjWtfiS5p",
        "colab_type": "text"
      },
      "source": [
        "# **Enable TPU for Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2-A3AVeiw5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "25327c38-669f-41f6-ccff-665fda2ea98b"
      },
      "source": [
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'Did you forget to switch to TPU?'\n",
        "tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR'] # colab is using grpc for its VPSes\n",
        "print(f\"Found TPU at {tpu_address}\")\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at grpc://10.51.196.66:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.51.196.66:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.51.196.66:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuAk6M5vReKG",
        "colab_type": "text"
      },
      "source": [
        "# **Tooling Parameters**\n",
        "\n",
        "[difference between cross entropy and kl divergence](https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence)\n",
        "\n",
        "[kullback leibler divergence explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)\n",
        "\n",
        "**Q**: where the formula of calculating `KL_loss` comes from?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJEfo1z8RwHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class TPARAM:\n",
        "    stage = 1\n",
        "    loss = ['binary_crossentropy']\n",
        "    optimizer = lambda name : Adam(lr=0.0002, beta_1=0.5, beta_2=0.999) if name == 'Adam' else RMSprop(learning_rate=0.0008, rho=1.0, decay=6e-8) \n",
        "    conditioned_text_embedding_dim = 128\n",
        "    noise_variable_dim = 100\n",
        "    batch_size = 64\n",
        "    buffer_size = 10000 # should be greater than or equal to the full size of the dataset\n",
        "    epochs = 1000\n",
        "    depth = 512\n",
        "    img_size = lambda stage : (64, 64) if stage == '1' else (256, 256)\n",
        "    ds_status = 'TRAIN'\n",
        "    metrics = ['accuracy']\n",
        "    ds_path = '/gdrive/My Drive/Birds-Dataset/'\n",
        "\n",
        "\n",
        "    def Model__save(self, dm, gm):\n",
        "        if type(dm) is 'Model' and type(gm) is 'Model':\n",
        "            dm.save('/gdrive/My Drive/GAN-models/stack__GAN/disc')\n",
        "            gm.save('/gdrive/My Drive/GAN-models/stack__GAN/gen')\n",
        "\n",
        "\n",
        "    def KL_loss(self, y_true, y_pred): # kl_loss for mean_logsigma distribution of adversarial network output on true text embeddings - operates on the latent tensor\n",
        "        ''' \n",
        "        With KL divergence we can calculate exactly how much information is lost when we approximate one distribution with another.\n",
        "        Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another (true text embeddings and latent space of them).\n",
        "        '''\n",
        "        mean = y_pred[:, :TPARAM.conditioned_text_embedding_dim]\n",
        "        logsigma = y_pred[:, :TPARAM.conditioned_text_embedding_dim]\n",
        "        loss = -logsigma + .5 * (-1 + tfx.math.exp(2. * logsigma) + tf.math.square(mean))\n",
        "        return tf.math.reduce_mean(loss)\n",
        "         \n",
        "\n",
        "    def plog__it(self):\n",
        "        # loading the history pickle files\n",
        "        with open('/gdrive/My Drive/GAN-models/stack__GAN/genLoss.pickle', 'rb') as fgen:\n",
        "            genHis = pickle.load(fgen, encoding='latin1')\n",
        "        with open('/gdrive/My Drive/GAN-models/stack__GAN/discLoss.pickle', 'rb') as fdis:\n",
        "            discHis = pickle.laod(fdisc, encoding='latin1')\n",
        "        \n",
        "        # https://plotly.com/~jefflai108/3/#/\n",
        "        # https://plotly.com/~jefflai108/5/#/\n",
        "        \n",
        "        # dict_of_fig = dict({\n",
        "        #     \"data\": [{\"type\": \"bar\",\n",
        "        #             \"x\": [1, 2, 3],\n",
        "        #             \"y\": [1, 3, 2]}],\n",
        "        #     \"layout\": {\"title\": {\"text\": \"Generator Loss\"}}\n",
        "        # })\n",
        "\n",
        "        # fig = go.Figure(dict_of_fig)\n",
        "        # fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gmoBiq4p5pO",
        "colab_type": "text"
      },
      "source": [
        "# **Image Processing Kit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dxw--tFqB57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def resize_img(img_path, bbox, img_size):\n",
        "    width, height = img_size\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Ex - bbox : 60.0 27.0 325.0 304.0 \n",
        "    # R : 325\n",
        "    # center_x = 282\n",
        "    # center_y = 179\n",
        "    # y1 = 0\n",
        "    # y2 = 504\n",
        "    # x1 = 0\n",
        "    # x2 = 607\n",
        "\n",
        "    if bbox is not None: # bbox cropping algorithm - loads the image and crops it around the provided bounding box \n",
        "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75) # bbox[2] : width, bbox[3] : height\n",
        "        center_x = int((2*bbox[0] + bbox[2]) / 2) # bbox[0] : x\n",
        "        center_y = int((2*bbox[1] + bbox[3]) / 2) # bbox[1] : y\n",
        "        y1 = np.maximum(0, center_y - R)\n",
        "        y2 = np.maximum(height, center_y + R)\n",
        "        x1 = np.maximum(0, center_x - R)\n",
        "        x2 = np.maximum(width, center_x + R)\n",
        "        img = img.crop([x1, y1, x2, y2]) # crop based on left, upper, right, and lower pixel coordinate\n",
        "        \n",
        "    img = img.resize(img_size, Image.BILINEAR)\n",
        "    return img\n",
        "\n",
        "def RGB_IMG_save(noise, epoch, nth_img):\n",
        "    if not os.path.isdir('generated'): os.mkdir('generated')\n",
        "    fig = px.imshow(noise * 127.5 + 127.5) # turin generated noise pixel into range 0 to 255 \n",
        "    fig.update_layout(title=f\"{epoch}-{nth_img}\", width=400, height=400)\n",
        "    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
        "    fig.write_image(f\"generated/sgan_{epoch}-{nth_img}.png\")\n",
        "\n",
        "\n",
        "def Make_Gif(self):\n",
        "    filenames = [fname for fname in np.sort(os.listdir('generated')) if \".png\" in fname]\n",
        "    with imageio.get_writer('generated/sgan.gif', mode=\"I\") as writer: # open a writer object for writing images on it to export a gif\n",
        "        for filename in filenames: # for every file in filenames list read them\n",
        "            image = imageio.imread('generated/'+filename)\n",
        "            writer.append_data(image) # append opened image into writer object for making gif\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fypucxf2LUE2",
        "colab_type": "text"
      },
      "source": [
        "# **Preparing CUB-Brids Dataset**\n",
        "\n",
        "**NOTE:** *Remember to merge folders after extracting each one and rename it to `Birds-Dataset` then run the following code cell.*\n",
        "\n",
        "[CUB-200-2011](https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE)\n",
        "\n",
        "[images](http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxChbo8bOetO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def load_class_ids(path, type):\n",
        "\twith open(path, 'rb') as f:\n",
        "\t\tclassIDs = pickle.load(f, encoding='latin1') # encoding='latin1' is required for unpickling numPy arr\n",
        "\tnp.save(f'{TPARAM.ds_path}classes_id-{type}.npy', classIDs) # 8855/2933 trianing/testing image class labels - ids\n",
        "\n",
        "\n",
        "def load_embeds(path, type):\n",
        "\twith open(path, 'rb') as f:\n",
        "\t\tembeds = pickle.load(f, encoding='latin1')\n",
        "\tnp.save(f'{TPARAM.ds_path}embeddings-{type}.npy', embeds) # 8855/2933 trianing/testing embeddings with 10 words padded in each sample and 1024 length of each word\n",
        "\n",
        "\n",
        "def load_fnames(path, type):\n",
        "\twith open(path, 'rb') as f:\n",
        "\t\tfnames = pickle.load(f, encoding='latin1')\n",
        "\tnp.save(f'{TPARAM.ds_path}file_names-{type}.npy', fnames) # 8855/2933 trianing/testing file with their names\n",
        "\n",
        "\n",
        "def load_bbox(bbox_path, file_path):\n",
        "\tdf_bounding_boxes = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)\n",
        "\tdf_filenames = pd.read_csv(file_path, delim_whitespace=True, header=None)\n",
        "\t\n",
        "\timg_filenames = df_filenames[1].tolist() # append all image file names into a list - df_filenames[0] => ids | df_filenames[1] => filenames - 11788 images in total\n",
        "\tfilename_bbox_dict = {img_file[:-4] : [] for img_file in img_filenames[:2]} # we're removing the jpg extension by :-4 syntax - we fill dictionary with only two first images cause we can't initialize an empty dict at the first step\n",
        "\t\n",
        "\tfor i in range(0, len(img_filenames)):\n",
        "\t\tbbox = df_bounding_boxes.iloc[i][1:].tolist() # we don't want the image id so we start from iloc[i][1:] then we append all those params into a list\n",
        "\t\tkey = img_filenames[i][:-4] # get the ith image name without the extension\n",
        "\t\tfilename_bbox_dict[key] = bbox # fill the above dictionary ----- key : filename , value : bbox - bounding boxes are used to extract objects from the raw images\n",
        "\t\n",
        "\tfn_bbox_dict = open(f\"{TPARAM.ds_path}file_name_bounding_boxes.pickle\",\"wb\") # saving the fname_bbox dict into a pickle file \n",
        "\tpickle.dump(filename_bbox_dict, fn_bbox_dict)\n",
        "\tfn_bbox_dict.close()\n",
        " \n",
        "\n",
        "load_class_ids(f'{TPARAM.ds_path}train/class_info.pickle', 'TRAIN')\n",
        "load_class_ids(f'{TPARAM.ds_path}test/class_info.pickle', 'TEST')\n",
        "\n",
        "load_embeds(f'{TPARAM.ds_path}train/char-CNN-RNN-embeddings.pickle', 'TRAIN')\n",
        "load_embeds(f'{TPARAM.ds_path}test/char-CNN-RNN-embeddings.pickle', 'TEST')\n",
        "\n",
        "load_fnames(f'{TPARAM.ds_path}train/filenames.pickle', 'TRAIN')\n",
        "load_fnames(f'{TPARAM.ds_path}test/filenames.pickle', 'TEST')\n",
        "\n",
        "load_bbox(f'{TPARAM.ds_path}bounding_boxes.txt', f'{TPARAM.ds_path}images.txt')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWnSQRN9BAsw",
        "colab_type": "text"
      },
      "source": [
        "# **Building Dataset**\n",
        "\n",
        "**NOTE:** *First of all create a folder called `final_data` then run following code cell where you have the `images` folder, to build the required `npy` files from `images`, `classes_ids` and `embeddings` tensors for training and testing data.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXj8tI1MBRyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be6bee97-359b-4b67-c69a-e257890d7dc0"
      },
      "source": [
        "\n",
        "def build_ds():\n",
        "    file_names = np.load(f'{TPARAM.ds_path}file_names-{TPARAM.ds_status}.npy') # numpy arr\n",
        "    classes_id = np.load(f'{TPARAM.ds_path}classes_id-{TPARAM.ds_status}.npy') # numpy arr\n",
        "    all_embeddings = np.load(f'{TPARAM.ds_path}embeddings-{TPARAM.ds_status}.npy') # numpy training shape : (8855, 10, 1024) - we have the word2vec tensor!\n",
        "\n",
        "    with open('file_name_bounding_boxes.pickle', 'rb') as f:\n",
        "        file_name_bounding_boxes = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    images_stage1, images_stage2, class_ids, embeddings = [], [], [], []\n",
        "\n",
        "    for index, fname in tqdm(enumerate(file_names)): # 8855 training images - in every iteration we pick a random embedding vector of size 1024 from 10 words of each sample, so our embeddings shape will be (8855, 1024)\n",
        "        bbox = file_name_bounding_boxes[fname]\n",
        "\n",
        "        try:\n",
        "            img_full_path = f'{TPARAM.ds_path}images/{fname}.jpg' # fname : images/<directory/species_name>/<image_name>\n",
        "            image_stage1 = resize_img(img_full_path, bbox, TPARAM.img_size(stage='1'))\n",
        "            image_stage2 = resize_img(img_full_path, bbox, TPARAM.img_size(stage='2'))\n",
        "            \n",
        "            # we do this because we want to concatenate text embeddings with generated image for discriminating and we can't the concatenate tensors with size (8855, 10, 1024) and (8855, 64, 64, 3)\n",
        "            # we can concatenate the latent space of text embeddings tensor with size (None, 4, 4, 128) generated from compress_embeddings function and the latent space of generated image using generator model with size (None, 4, 4, 512)\n",
        "            embed_idx = np.random.randint(0, all_embeddings[index, :, :].shape[0] - 1) # select a random index between 0 and the number of rows (words) in each sample\n",
        "            embeddings.append(all_embeddings[index, embed_idx, :]) # our vocab training shape : (8855, 1024) - append a random embedding vector from all embeddings of indexTh sample to the embedding list with size 1024\n",
        "            images_stage1.append(np.array(image_stage1))\n",
        "            images_stage2.append(np.array(image_stage1))\n",
        "            class_ids.append(classes_id[index]) # append the class id for indexTh image to the class_ids list \n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'[An Error is going to kill us! ::: {e}]')\n",
        "\n",
        "    images_stage1 = np.save(f'{TPARAM.ds_path}final_data/images_stage1-{TPARAM.ds_status}.npy', np.array(images_stage1))\n",
        "    images_stage2 = np.save(f'{TPARAM.ds_path}final_data/images_stage2-{TPARAM.ds_status}.npy', np.array(images_stage2))\n",
        "    class_ids = np.save(f'{TPARAM.ds_path}final_data/classes_id-{TPARAM.ds_status}.npy', np.array(class_ids))\n",
        "    embeddings = np.save(f'{TPARAM.ds_path}final_data/embeddings-{TPARAM.ds_status}.npy', np.array(embeddings))\n",
        "\n",
        "\n",
        "build_ds()\n",
        "TPARAM.ds_status = 'TEST'\n",
        "build_ds()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8855it [00:00, 88199.75it/s]\n",
            "2933it [00:00, 108588.60it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl8RWHpcMWhG",
        "colab_type": "text"
      },
      "source": [
        "# **$** \n",
        "**Loading Dataset Through Tensorflow Input Pipeline**\n",
        "\n",
        "[meaning of buffer size in dataset](https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7FWx9vSMa2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "images_stage1 = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/images_stage1-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing images for stage1 \n",
        "images_stage2 = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/images_stage2-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing images for stage2\n",
        "embeddings = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/embeddings-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing pre-trained text embeddings of size 1024\n",
        "classes_id = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/classes_id-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing classes id\n",
        "\n",
        "if TPARAM.ds_status == 'TRAIN': # create a buffer of at most TPARAM.buffer_size elements, and a background thread to fill that buffer in the background.\n",
        "    images_stage1 = images_stage1.cache() # allows to cache elements of the dataset for future reusing. Cached data will be store in memory (by default)\n",
        "    images_stage1 = images_stage1.shuffle(TPARAM.buffer_size, reshuffle_each_iteration=True) # every time when data was needed, it takes from the buffer. After that buffer is filled up with newest elements to the given buffer size and also shuffle order should be different for each epoch with the last flag argument. \n",
        "    images_stage1 = images_stage1.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE) # num_parallel_calls should be equal the number of processes that can be used for transformation. tf.data.experimental.AUTOTUNE defines appropriate number of processes that are free for working - preprocessing level ... scale to [-1, 1]\n",
        "    images_stage1 = images_stage1.batch(TPARAM.batch_size, drop_remainder=True) # drop the last batch cause it doesn't fit the batch size - 8855 images devided into 8855/HPARAM.batch_size batches each of size HPARAM.batch_size\n",
        "    images_stage1 = images_stage1.prefetch(tf.data.experimental.AUTOTUNE) # defines appropriate number of batches to feed into the next iteration and the maximum number of elements that will be buffered when prefetching - prevent CPU stands idle and allows later elements to be prepared while the current element is being processed.\n",
        "    images_stage1 = list(images_stage1.as_numpy_iterator()) # shuffled , preprocessed and batched\n",
        "\n",
        "    images_stage2 = images_stage2.cache() \n",
        "    images_stage2 = images_stage2.shuffle(TPARAM.buffer_size, reshuffle_each_iteration=True)\n",
        "    images_stage2 = images_stage2.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    images_stage2 = images_stage2.batch(TPARAM.batch_size, drop_remainder=True)\n",
        "    images_stage2 = images_stage2.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    images_stage2 = list(images_stage2.as_numpy_iterator()) # shuffled , preprocessed and batched\n",
        "\n",
        "    embeddings = embeddings.cache()\n",
        "    embeddings = embeddings.shuffle(TPARAM.buffer_size, reshuffle_each_iteration=True)\n",
        "    embeddings = embeddings.batch(TPARAM.batch_size, drop_remainder=True)\n",
        "    embeddings = embeddings.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    embeddings = list(embeddings.as_numpy_iterator()) # shuffled and batched\n",
        "\n",
        "if TPARAM.ds_status == 'TEST':\n",
        "    images_stage1 = images_stage1.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    images_stage1 = list(images_stage1.as_numpy_iterator())\n",
        "\n",
        "    images_stage2 = images_stage2.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    images_stage2 = list(images_stage2.as_numpy_iterator())\n",
        "\n",
        "    embeddings = list(embeddings.as_numpy_iterator())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUyS0oL9FbAT",
        "colab_type": "text"
      },
      "source": [
        "# **Conditional Augmentation Kit**\n",
        "\n",
        "**Q**: where the formula of calculating `c` comes from?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBeGyDuaFl9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def conditioned(x): # generate conditional augmentation on text embeddings - x shape : (None, 256)\n",
        "    '''\n",
        "    x is the output of a feeded text embeddings into a fully connected dense layer to calculate the mean (μ(φt)) and standard deviation (σ(φt)) by splitting it into two tensors from mean_logsigma output.\n",
        "    N(μ(φt), Σ(φt)) : gaussian distribution of standard deviations (diagonal of covariance matrix = Σ(φt)) and mean of embedded texts - tf.random.normal(observations, mean, standard_deviation)\n",
        "    we have to calculate the value of c using above distribution and the formula : mean + standard deviation * epsilon (N(0, I)) which is a normal distribution.\n",
        "    both μ(φt) and Σ(φt) are learned jointly with the rest of the network in generator model during back propagation.\n",
        "    =========================================\n",
        "    logvar = log_sigma\n",
        "        it makes sense that:\n",
        "    logvar.exp() = sigma\n",
        "        Mathematically it is all equivalent.\n",
        "    '''\n",
        "    mean = x[:, :TPARAM.conditioned_text_embedding_dim] # μ(φt) - the mean of our feeded text embeddings - output shape : all rows and the first 128 columns\n",
        "    log_sigma = x[:, TPARAM.conditioned_text_embedding_dim:] # σ(φt) - the diagonal covariance matrix of our feeded text embeddings - output shape : all rows and the second 128 columns \n",
        "    standard_deviation = K.exp(log_sigma) # calculate the diagonal of covariance matrix = Σ(φt) which is the standard deviations (σ) - to calculate the sigma or standard deviation we have to pass its log through an exp function\n",
        "    epsilon = K.random_normal(shape=K.constant((mean.shape[1], ), dtype='int32')) # generate an epsilon normal distribution between 0 to 128\n",
        "    return standard_deviation * epsilon + mean # we use this variable for both stages with size (None, 128)\n",
        "\n",
        "def conditional_augmentation_model(): # for generator second output\n",
        "    '''\n",
        "    In the this network, the text embedding vector is passed through a fully connected layer with nonlinearity, which produces the mean μ(φt) and the diagonal covariance matrix = Σ(φt).\n",
        "    the diagonal elements of the covariance matrix = Σ(φt) are the variances and the square root of these variances are the standard deviations (σ).\n",
        "    '''\n",
        "    our_input = Input(shape=(1024,)) # (None, 1024)\n",
        "    mean_logsigma = Dense(256)(our_input) # (None, 256)\n",
        "    mean_logsigma = LeakyReLU(0.2)(mean_logsigma) # mean_logsigma - predict the mean and standard deviation (the square root of diagonal covariance matrix) - output of a dense layer is mu and logsigma\n",
        "    # c = Lambda(conditioned)(mean_logsigma)\n",
        "    return Model(our_input, mean_logsigma) # (None, 1024) -> (None, 256)\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j4-MaHBFrvO",
        "colab_type": "text"
      },
      "source": [
        "# **Compressing Text Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt9AV_muFySA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def compress_embeddings__(): # for stage1 discriminator second input\n",
        "    '''\n",
        "    compress the embedding samples to a latent space with shape 128.\n",
        "    giving a tensor of size 1024 and returning a latent space of that tensor with size 128.\n",
        "    '''\n",
        "    embeddings_input = Input(shape=1024,)\n",
        "    output = Dense(TPARAM.conditioned_text_embedding_dim)(embeddings_input)\n",
        "    output = ReLU()(output)\n",
        "    return Model(embeddings_input, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rtriJnMiccS",
        "colab_type": "text"
      },
      "source": [
        "# **Generating 64 x 64 Images using a Random Noise Variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ez8jalOKkVmQ"
      },
      "source": [
        "![gen architecture](https://drive.google.com/uc?id=1NctCmLFct6LgT9jsT9R4Mz7TDm5MjgXA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu3s7HP4VaKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generator():\n",
        "    '''\n",
        "    conditioned on the text embeddings with 1024 features, It takes a random noise vector sampled from a latent space and generates an image with a shape of 64x64x3.\n",
        "    we use 128 * 8 depth and a 4 x 4 matrix of image size for the starter layers to reach the 64 x 64 image size and a half of the 128 * 8 depth in each iteration.  \n",
        "    '''\n",
        "    print(\"\\n\\n===================== GENERATOR SUMMARY =====================\\n\\n\")\n",
        "    first_input = Input(shape=(1024,)) # text embeddings with size (None, 1024) - for training the size is (8855, 1024)\n",
        "    mean_logsigma = Dense(256)(first_input) # latent space of text embeddings with size (None, 256)\n",
        "    mean_logsigma = LeakyReLU()(mean_logsigma) # mean_logsigma (μ(φt) and Σ(φt)) are learned jointly during back propagation using KL_loss function to update the text conditioned variable\n",
        "    c = Lambda(conditioned)(mean_logsigma) # the latent space (output) size : (None, 128) - conditioned on text embeddings\n",
        "    \n",
        "    second_input = Input(shape=(TPARAM.noise_variable_dim,)) # random noise varibale with size (None, 100)\n",
        "    merged_inputs = Concatenate(axis=1)([c, second_input]) # output size : (None, 228) = (None, 100) + (None, 128) - concatenating the generated condition on text and the generated noise (None, 100) from a normal distribution along the columns axis\n",
        "    \n",
        "    TPARAM.depth = 512\n",
        "    output = Dense(int(TPARAM.depth/4) * 8 * 4 * 4, use_bias=False)(merged_inputs) # output size : (None, 16384) - we use 16384 columns to reshape it simply\n",
        "    output = ReLU()(output)\n",
        "    output = Reshape((4, 4, int(TPARAM.depth/4) * 8))(output) # reshape the concatenated value to 4, 4, 1024 to double the size till we reach the image size - output size : (None, 4, 4, 1024)\n",
        "    \n",
        "    for i in range(4): # we need 4 Conv2D layer to half the filter size from 1024 and double the image size in each iteration\n",
        "        output = UpSampling2D((2,2))(output)\n",
        "        output = Conv2D(TPARAM.depth, kernel_size=(3,3), padding=\"same\", strides=1, use_bias=False)(output)\n",
        "        output = BatchNormalization()(output)\n",
        "        output = ReLU()(output)\n",
        "        TPARAM.depth = int(TPARAM.depth / 2)\n",
        "    \n",
        "    output = Conv2D(3, kernel_size=(3,3), padding=\"same\", strides=1, use_bias=False)(output) # output size : (None, 64, 64, 3) - generated image\n",
        "    output = Activation(activation='tanh')(output) # output is [-1, 1]\n",
        "\n",
        "    TPARAM.depth = int(512/8)\n",
        "    generator_model = Model([first_input, second_input], [output, mean_logsigma]) # with two inputs we'll have two tensors as our total output - the mean_logsigma output is just a tensor that help us to know how much changes should apply to our text condition variable during back propagation in generator and sgan model.  \n",
        "    # plot_model(generator_model, to_file='generator_model.png')\n",
        "    # generator_model.summary()\n",
        "    return generator_model # return a model with two inputs : noise and the conditioned text embedding & two outputs : the generated image and a latent space of compressed text embeddings with size 256 \n",
        "\n",
        "# generator()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFbhChA-uUMd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtFN8Oq7jKXu",
        "colab_type": "text"
      },
      "source": [
        "# **Discriminating Generated Images Combined with the Latent Space of Text Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMcHA6iQkC4U",
        "colab_type": "text"
      },
      "source": [
        "![disc architecture](https://drive.google.com/uc?id=1KBh0cX85D2EJ5VNYuxxZytasjtKguzns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L57e0srNkC1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def discriminator():\n",
        "    '''\n",
        "    this network contains a set of downsampling layers and classifies whether a given image is real or fake.\n",
        "    this model takes two inputs : the generated image of size 64 x 64 and the spatial replicated of compressed text embedding with size 128.\n",
        "    the second input is the half of the second output of generator network, the latent space of compressd text embeddings with size 128 (256 / 2) \n",
        "    '''\n",
        "    print(\"\\n\\n===================== DISCRIMINATOR SUMMARY =====================\\n\\n\")\n",
        "    first_input = Input(shape=(64, 64, 3)) # the generated image\n",
        "    second_input = Input(shape=(4, 4, TPARAM.conditioned_text_embedding_dim)) # latent space of text embeddings as the second input for discriminator - we use this size that the latent space of text embeddings be concatenable with image size : (None, 4, 4, 128) + (None, 4, 4, 512) = (None, 4, 4, 640)\n",
        "    \n",
        "    output = Conv2D(TPARAM.depth, kernel_size=(4,4), padding=\"same\", strides=2, use_bias=False)(first_input) # output size : (None, 32, 32, 64)\n",
        "    output = LeakyReLU(0.2)(output)\n",
        "\n",
        "    for i in range(3): # 3 iterations to half the image size and double the filter size to reach the 512 for concatenating with latent space of text embeddings with size 128 for a 640 (128 + 512) as a concatenating size and 4 for latent space size\n",
        "        TPARAM.depth *= 2\n",
        "        output = Conv2D(TPARAM.depth, kernel_size=(4, 4), padding='same', strides=2, use_bias=False)(output)\n",
        "        output = BatchNormalization()(output)\n",
        "        output = LeakyReLU(0.2)(output)\n",
        "    \n",
        "    TPARAM.depth = int(512/8)\n",
        "    \n",
        "    merged_inputs = concatenate([output, second_input]) # output size : (None, 4, 4, 128) + (None, 4, 4, 512) = (None, 4, 4, 640) - merging the output of discriminating the generated image and the latent space of text embeddings - compression and spatial replication of the image and text embeddings\n",
        "    second_output = Conv2D(TPARAM.depth * 8, kernel_size=(1,1), padding=\"same\", strides=1)(merged_inputs) # passing the merged_inputs through a convolutional layer to jointly learn the features across the image and the text\n",
        "    second_output = BatchNormalization()(second_output)\n",
        "    second_output = LeakyReLU()(second_output)\n",
        "    second_output = Flatten()(second_output) # flatten the output of the previous layer\n",
        "    second_output = Dense(1)(second_output) # output size : (None, 1)\n",
        "    second_output = Activation('sigmoid')(second_output) # output is [0, 1]\n",
        "    \n",
        "    discriminator_model = Model([first_input, second_input], [second_output])\n",
        "    # plot_model(discriminator_model, to_file='discriminator_model.png')\n",
        "    # discriminator_model.summary()\n",
        "    return discriminator_model # return a model with two inputs : the generated image and the latent space of the text embeddings with one output : the discriminated text and the generated image\n",
        "\n",
        "# discriminator()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjEhroE5pHj",
        "colab_type": "text"
      },
      "source": [
        "# **Building Aversarial Network**\n",
        "\n",
        "[what those values in \"Connected to\" column of model summary mean?](https://stackoverflow.com/questions/53942291/what-does-the-00-of-the-layers-connected-to-in-keras-model-summary-mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBEzpGQkjdS",
        "colab_type": "text"
      },
      "source": [
        "![sgan network](https://drive.google.com/uc?id=1iaY04XMFd_94upioRo7afgsSfyiLrju-)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfEPf0z54Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def adversarial_network(gen, disc):\n",
        "    print(\"\\n\\n===================== STACKGAN SUMMARY =====================\\n\\n\")\n",
        "    embedding_input_for_generator = Input(shape=(1024,)) # training text embeddings\n",
        "    noise_input_for_generator = Input(shape=(TPARAM.noise_variable_dim,)) # random noise variable\n",
        "    latent_space_of_text_embeddings = Input(shape=(4, 4, TPARAM.conditioned_text_embedding_dim)) # latent space of text embeddings to merge with down sampled generated image with size (None, 4, 4, 512) for discriminating\n",
        "\n",
        "    generated_image, mean_logsigma = gen([embedding_input_for_generator, noise_input_for_generator]) # two inputs : training text embeddings and random noise variable - two outputs : generated image and latent space of text embeddings with size (None, 256)\n",
        "    disc.trainable = False # freez all discriminator weights when we're training the generator, we should do this before compiling the model\n",
        "    valid_output = disc([generated_image, latent_space_of_text_embeddings]) # two inputs : the generated image and the latent space of text embeddings with size (None, 4, 4, 128) to discriminate the image in a combined normal distribution with latent space of text embeddings\n",
        "\n",
        "\n",
        "    # texts, noise varaible tensors in a combined normal distribution & latent space of text embeddings -> sgan_model -> generated image validation & latent space of text embeddings with size (None, 256)\n",
        "    # for our adversarial model in stage1 architecture we should have two outputs : the discriminated images-texts and the latent space of text embeddings with size (None, 256)\n",
        "    # the generator will maximize the loss of the discriminator on discriminating its output through the discriminator in sgan model by minimizing the adversarial loss so our gen loss is the adversarial loss cause we don't have labels in generator model itself.\n",
        "    # our second output mean_logsigma is a variable that we calculate the text condition based on; and because of that we have to put it in our sgan output to know how much we have to change the text condition variable respect to mean_logsigma latent space during back propagation of generator network.\n",
        "    sgan_model = Model([embedding_input_for_generator, noise_input_for_generator, latent_space_of_text_embeddings], [valid_output, mean_logsigma])\n",
        "    \n",
        "    # sgan_model.summary()\n",
        "    # plot_model(sgan_model, to_file='sgan_model.png')\n",
        "    return sgan_model\n",
        "\n",
        "\n",
        "# adversarial_network(generator(), discriminator())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYI1NBUhbdGG",
        "colab_type": "text"
      },
      "source": [
        "# **Building, Compiling & Training Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vWKk7fzbfxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "optimizers are algorithms or methods used to change the attributes of \n",
        "your neural network such as weights and learning rate in order to reduce the losses.\n",
        "optimization algorithms or strategies are responsible for reducing the \n",
        "losses and to provide the most accurate results possible\n",
        "'''\n",
        "\n",
        "with strategy.scope():\n",
        "    ca_model = conditional_augmentation_model()\n",
        "    ca_model.compile(loss=TPARAM.loss, optimizer=TPARAM.optimizer(\"Adam\")) # compiling the conditional augmentation model which its output size is (None, 256)\n",
        "\n",
        "    gen_model = generator()\n",
        "    gen_model.summary()\n",
        "    gen_model.compile(loss=\"mse\", optimizer=TPARAM.optimizer(\"Adam\")) # compiling the generator model to set the loss and optimizer parameters - mean_sqaured_error is a loss function for generated_image and original_image\n",
        "\n",
        "    disc_model = discriminator()\n",
        "    disc_model.summary()\n",
        "    disc_model.compile(loss=TPARAM.loss, optimizer=TPARAM.optimizer(\"Adam\")) # binary_crossentropy is a classification loss function for 0 and 1 class\n",
        "\n",
        "    ce_model = compress_embeddings__()\n",
        "    ce_model.compile(loss=TPARAM.loss, optimizer=TPARAM.optimizer(\"Adam\")) # compiling the compress embeddings model which its out put size is (None, 128)\n",
        "\n",
        "\n",
        "    '''\n",
        "    optional list or dictionary representing scalar coefficients (Python floating point numbers) for weighting the loss contributions of different model outputs.\n",
        "    The loss value minimized by the model is loss_weights a weighted sum of individual losses weighted by a coefficient.\n",
        "    loss_weights parameter on compile is used to define how much each of your model output loss contributes to the final loss value.\n",
        "    it weighs the model output losses. You could have a model with 2 outputs where one is the primary output and the other auxiliary.\n",
        "    \n",
        "    the first output contributes 1 time to the final loss (1 * final loss for first output loss) and the second one which is the mean_logsigma contributes 2 times \n",
        "    to the final (2 * final loss for second output loss) loss which has more contribution than the first one.\n",
        "    total loss = (1 * final loss for first output loss) + (2 * final loss for second output loss)\n",
        "    \n",
        "    in order to train the generator we have to train the sgan model on batches cause we don't have labels in generator model and we are comparing \n",
        "    the discriminators decisions on the generated images and the generator tries to maximize the discriminator's output for its fake instances.\n",
        "    \n",
        "    compiling adversarial model : text embeddings & noise -> gen -> generated noise & latent space of text embeddings -> disc -> valid & mean_logsigma\n",
        "    we used KL_loss for the second output mean_logsigma of our sgan model\n",
        "    '''\n",
        "    sgan_model = adversarial_network(gen=gen_model, disc=disc_model)\n",
        "    sgan_model.summary()\n",
        "    sgan_model.compile(loss=[\"binary_crossentropy\", TPARAM.kl_loss], loss_weights=[1, 2.0], optimizer=TPARAM.optimizer(\"Adam\"), metrics=None)\n",
        "\n",
        "\n",
        "real_labels = tf.ones((TPARAM.batch_size, 1), dtype='float32') * .9 # creating true smoothed labels for our total batch size for training data\n",
        "fake_labels = tf.zeros((TPARAM.batch_size, 1), dtype='float32') * .1 # creating fake smoothed labels for our totla batch size for generated noises \n",
        "\n",
        "print()\n",
        "\n",
        "for epoch in range(TPARAM.epochs):\n",
        "    print(f\"__________________________________________________________ Epoch {epoch+1} __________________________________________________________\")\n",
        "    print(f\"[INFO] Total number of batches for 8855 training images with {TPARAM.batch_size} samples in each batch : {len(images_stage1)}\") # the images_stage1 is total training images separated into batches and the length of images_stage1 input pipeline list is the number of batches. \n",
        "\n",
        "    generator_losses, discriminator_losses = [], [] # save gen and disc losses for every epoch\n",
        "    for batch in range(len(images_stage1[0])): # we're gonna train our models for every batch in our training datasets (images_stage1 & embeddings)\n",
        "        print(f\"[INFO] Batch : {batch+1}/{TPARAM.batch_size}\")\n",
        "        z_noise_var = tf.random.normal((TPARAM.batch_size, TPARAM.noise_variable_dim), 0, 1) # creating a random noise variable from a normal distribution\n",
        "        img_batch = images_stage1[np.random.randint(0, len(images_stage1))] # select a random batch of images pipline\n",
        "        img_batch = 0.5 * img_batch + 0.5 # rescale to [0, 1] - because all training images have range [-1, 1] and to feed the batch into the discriminator network we have to scale our data to [0, 1]\n",
        "        embd_batch = embeddings[np.random.randint(0, len(embeddings))] # select a random batch of embeddings pipline\n",
        "\n",
        "\n",
        "        # our generator takes two inputs : the embedding batch of texts and random noise variable to generate a text conditioned variable with size (batch, 128)\n",
        "        # based on the latent space with 256 features of text embeddings to combine it with the random noise variable with size (TPARAM.batch_size, 100); \n",
        "        # the mean_logsigma will be use and update in generator during backpropagation (because it's a output of generator and our adversarial network and it'll update with loss functions) \n",
        "        # to update the conditioned variable for generating the the latent space of text embeddings with size (batch, 128) for discriminator input.\n",
        "        generated_noise, mean_logsigma = gen_model.predict([embd_batch, z_noise_var], verbose=1)\n",
        "        generated_noise = 0.5 * generated_noise + 0.5 # scale the image which is between -1 and 1 to 0 and 1 - because the output of discriminator is [0, 1] and we have to scale our input data for the network\n",
        "        compress_embeddings = ce_model.predict_on_batch(embd_batch) # output size : (batch, 128) - generate a compressed embeddings of a single batch of texts with 128 features for discriminator input to merge with the generated image to discriminate both of them together.\n",
        "\n",
        "\n",
        "        # because the default shape of compress_embeddings predicted by its model is (batch, 128) we have to reshape our compress_embeddings \n",
        "        # to make its shape suitable for discriminator input cause the discriminator takes a latent space with size (batch, 4, 4, 128)\n",
        "        # here -1 means the batch size and not the batch * TPARAM.conditioned_text_embedding_dim cause we have TPARAM.conditioned_text_embedding_dim in 4th dim\n",
        "        # there is one row of each (1, TPARAM.conditioned_text_embedding_dim) and batch size row of the whole\n",
        "        compress_embeddings = np.reshape(compress_embeddings, (-1, 1, 1, TPARAM.conditioned_text_embedding_dim)) # output size : (batch, 1, 1, TPARAM.conditioned_text_embedding_dim)\n",
        "        compress_embeddings = np.tile(compress_embeddings, (1, 4, 4, 1)) # output size : (batch, 4, 4, TPARAM.conditioned_text_embedding_dim) - tile the input with (1, 4, 4, 1) repetitions to get the desired size for discriminator input\n",
        "        disc_model.trainable = True # unfreeze the discriminator weights for training\n",
        "\n",
        "\n",
        "        # train_on_batch method returns a scalar training loss if the model has no metrics and a list of scalars ([0] is loss & [1] is accuracy) if model has metrics available in its compile method.\n",
        "        # our discriminator model takes two inputs (batch of real images and compress text embeddings that we just prepared)\n",
        "        # and the true labels of y to run a single gradient update on the image and compress embeddings batch of data\n",
        "        # below we have real images and their corresponding text as positive sample pairs with true labels.\n",
        "        disc_positive_sample_pairs_loss = disc_model.train_on_batch([img_batch, compress_embeddings], real_labels)\n",
        "\n",
        "\n",
        "        # generated images with their corresponding embeddings and fake labels\n",
        "        # returns the discriminating loss of a single batch of the fake images and compress embeddings respect to fake labels\n",
        "        disc_negative_sample_pairs_loss_v1 = disc_model.train_on_batch([generated_noise, compress_embeddings], fake_labels)\n",
        "        \n",
        "        \n",
        "        # real images with mismatched embeddings and fake labels - cut two row (image or slices) from the end of the img_batch and also first two rows (0 and 1 embeddings) \n",
        "        # from compress_embeddings to get real images with mismatched embeddings (ruin corresponding embeddings of real images with cutting slices); \n",
        "        # finally for the y labels (actual values) we have to cut the first two rows of fake_labels (ruin corresponding labels with size (batch, 1) \n",
        "        # of training data which is the real images with mismatched embeddings) and reshape to the number of real images that we just dropped its last two rows of it. \n",
        "        disc_negative_sample_pairs_loss_v2 = disc_model.train_on_batch([img_batch[:(TPARAM.batch_size - 2)], compress_embeddings[2:]], np.reshape(fake_labels[2:], (TPARAM.batch_size - 2, 1))) \n",
        "\n",
        "\n",
        "        disc_model.trainable = False # freeze the discriminator weights to train the generator        \n",
        "        Dloss = 0.5 * tf.math.add(disc_positive_sample_pairs_loss, 0.5 * tf.math.add(disc_negative_sample_pairs_loss_v1, disc_negative_sample_pairs_loss_v2)) # the total loss of discriminator : positive samples loss + negative samples loss\n",
        "        Gloss = sgan_model.train_on_batch([embd_batch, z_noise_var, compress_embeddings], [tf.ones((TPARAM.batch_size, 1)) * 0.9, tf.ones((TPARAM.batch_size, 256)) * 0.9]) # takes three inputs as training data and two smoothed tensors as y labels - because our adversarial network has two outputs we have to pass the true labels (real labels) for our data batch : one for valid and the other for mean_logsigma (latent space with size (batch, 256)) update during pack propagation with KL_loss\n",
        "\n",
        "        \n",
        "        # we didn't specify metrics in above models(disc, gen, sgan, ce, ca) and because of that the output of train_on_batch method \n",
        "        # on one of these models don't have a list of scalars; one scalar only which is the calculated loss on a single batch of data in each iteration. \n",
        "        print(f\"[INFO] discriminator loss of positive samples : {disc_positive_sample_pairs_loss}\")\n",
        "        print(f\"[INFO] discriminator loss of nagative samples - generated images with their correspond text embeddings : {disc_negative_sample_pairs_loss_v1}\")\n",
        "        print(f\"[INFO] discriminator loss of nagative samples - real images with mismatched text embeddings : {disc_negative_sample_pairs_loss_v2}\")\n",
        "        print(f\"[INFO] discriminator loss : {Dloss}\")\n",
        "        print(f\"[INFO] generator loss - adversarial loss with 3 input and 2 output tensors : {Gloss}\")\n",
        "        print(\"_______________________________________________________________________________________________________________\")\n",
        "\n",
        "        generator_losses.append({'generator loss': Gloss})\n",
        "        discriminator_losses.append({'discriminator loss': Dloss})\n",
        "\n",
        "        with open('/gdrive/My Drive/GAN-models/stack__GAN/genLoss.pickle', 'wb') as fgen:\n",
        "            pickle.dump(generator_losses, fgen)\n",
        "        with open('/gdrive/My Drive/GAN-models/stack__GAN/discLoss.pickle', 'wb') as fdisc:\n",
        "            pickle.dump(discriminator_losses, fdisc)\n",
        "\n",
        "    if epoch % 4 is 0: # save generated images every 4 epoch\n",
        "        z_noise_var__ = tf.random.normal((TPARAM.batch_size, TPARAM.noise_variable_dim), 0, 1) # generate noise for every batch with 100 features\n",
        "        embeddings__ = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/embeddings-TEST.npy'))) # 2933 testing pre-trained text embeddings of size 1024\n",
        "        embeddings__ = list(embeddings__.as_numpy_iterator())\n",
        "        embeddings_batch = np.asarray(embeddings__[0:TPARAM.batch_size]) # select a batch of test embeddings for testing the generator model\n",
        "        fake_images, mean_logsigma = gen_model.predict_on_batch([embeddings_batch, z_noise_var__]) # got two outputs - predict a single batch of data with test embeddings and a normal distribution of random noise variable\n",
        "\n",
        "        for nth_img, image in enumerate(fake_images[:25]): # first 25 images with nth_img as index\n",
        "            RGB_IMG_save(image, epoch, nth_img) # save first 25 generated images with unseen text data (testing text embeddings)\n",
        "\n",
        "\n",
        "TPARAM.Model__save(dm=isc_model, gm=gen_model) # save trained models\n",
        "TPARAM.plog__it(generator_losses, discriminator_losses)\n",
        "Make_Gif() # make gif of all generated images\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoD-G99BYZ4",
        "colab_type": "text"
      },
      "source": [
        "**StackGAN Model Prediction API for Stage1 - Tensorflow Serving**\n",
        "\n",
        "**NOTE**: *before testing remember to change the `ds_status` of `TPARAM` class to `TEST` and load the testing data into tensorflow pipleline by running the code of `$` cell*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJosRiYeQ-9N",
        "colab": {}
      },
      "source": [
        "\n",
        "# TODO : https://www.tensorflow.org/tfx/tutorials/serving/rest_simple\n",
        "# TODO : load the models, test it with testing data and build the prediction api\n",
        "# OUTPUT : high res image from a text\n",
        "\n",
        "\n",
        "# print(images_stage1)\n",
        "# print(images_stage2)\n",
        "# print(embeddings)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}