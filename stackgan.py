# -*- coding: utf-8 -*-
"""StackGAN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vN89zBmv_Q3PaQwMNLVvNEYGHKvEk0RP

# **UNSUPERVISED LEARNING | GANZ00 (The Art of Programming) _ Stack-GAN**

> The generator
networks in both stages are Conditional Generative Adversarial
Networks (CGANs). The first GAN is conditioned on the text
descriptions, while the second network is conditioned on the text
descriptions and the images generated by the first GAN as well
& binary-cross-entropy uses as our loss function in both stages.

***Powered by:***

![uniXerr logo](https://drive.google.com/uc?id=1TXJwfJsTJzU2M7LrIQgx2Tx4cfUzcQuX)

![gds](https://drive.google.com/uc?id=1Q1hdN4xJfcCfZNnmKU3sGfi64C-dWETX)

![sgan](https://drive.google.com/uc?id=1QkMK7mZFNeXsQXuszgXbowG_SQRgl2jj)

The architecture of the proposed StackGAN. The Stage-I generator draws a low-resolution image by sketching rough shape and
basic colors of the object from the given text and painting the background from a random noise vector. Conditioned on Stage-I results, the
Stage-II generator corrects defects and adds compelling details into Stage-I results, yielding a more realistic high-resolution image.

![sgan](https://drive.google.com/uc?id=1HMzOx6xnaWe2TtOACCVsLV4gjQMNebPR)

*   `t` This is a text description of the true data distribution.
*   `z` This is a randomly sampled noise vector from a Gaussian distribution.
*   `φt` This is a text embedding of the given text description
generated by a pre-trained encoder.
*   `Ĉ0` This text conditioning variable is a Gaussian conditioning
variable sampled from a distribution . It captures the
different meanings of.
*   `N(μ(φt), Σ(φt))` This is a conditioning Gaussian distribution.
*   `N(0, I)` This is a normal distribution.
*   `Σ(φt)` This is a diagonal covariance matrix.
*   `pdata` This is the true data distribution.
*   `Pz` This is the Gaussian distribution.
*   `D1` This is the Stage-I discriminator.
*   `G1` This is the Stage-I generator.
*   `D2` This is the Stage-II discriminator.
*   `G2` This is the Stage-II generator.
*   `N2` These are the dimensions of the random noise variable.
*   `Ĉ`  These are the Gaussian latent variables for the Stack-II GAN.

![symbol](https://drive.google.com/uc?id=1SrLUphhN566ECg9arCzRP418Yz7bM6pb)

![sgan-s1](https://drive.google.com/uc?id=1ahtUJcScvngZ-vpR_-uT6tRaL6AS6Kt5)

![sgan-s2](https://drive.google.com/uc?id=1srovKaptPDvohKkjCwyzrzjCu_02Z_y-)

![sgan-s2-c](https://drive.google.com/uc?id=1s5iK7yqcF_9onHqp_znrP5T8YSHjGtc5)

***StackGAN Papers & Codes:***

[AttnGAN - code](https://github.com/taoxugit/AttnGAN)

[StackGAN-v1 - paper](https://arxiv.org/pdf/1612.03242.pdf)

[StackGAN-v2 - code](https://github.com/hanzhanggit/StackGAN-v2)

[StackGAN-v1 - code](https://github.com/Vishal-V/StackGAN)

[StackGAN-v1-Stage2 - code](https://github.com/mrrajatgarg/StackGAN-Keras-implementation/blob/master/stackgan_stage_2_implementation.ipynb)

**TODO**

**1)** find the answers of questions. 

**2)** complete `TPARAM.plog__it` function. 

**3)** implement stage2 

**4)** complete training & prediction parts

# **Mount Google Drive**
"""

from google.colab import drive
drive.mount('/gdrive')

"""# **Requirements**"""

from __future__ import print_function, division
from PIL import Image
import pprint
import time
import random
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
import os
import sys
import imageio
import tensorflow as tf
import cv2
import asyncio
import math
import seaborn as sns
import tensorflow.keras.backend as K
from tqdm import tqdm
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Reshape
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import ZeroPadding2D
from tensorflow.keras.layers import LeakyReLU, ReLU
from tensorflow.keras.layers import UpSampling2D
from tensorflow.keras.layers import Conv2DTranspose
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Lambda
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import Multiply
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10

# Confirm that we're using Python 3
assert sys.version_info.major is 3, 'Switch to python3 please!'
print("[...] Installing dependencies for Colab environment\n\n")
!pip install plotly==4.6.0
!pip install -Uq grpcio==1.26.0
!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca
!chmod +x /usr/local/bin/orca
!apt-get install xvfb libgtk2.0-0 libgconf-2-4

"""# **Enable TPU for Training**"""

assert 'COLAB_TPU_ADDR' in os.environ, 'Did you forget to switch to TPU?'
tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR'] # colab is using grpc for its VPSes
print(f"Found TPU at {tpu_address}")

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)

"""# **Tooling Parameters**

[difference between cross entropy and kl divergence](https://stats.stackexchange.com/questions/357963/what-is-the-difference-cross-entropy-and-kl-divergence)

[kullback leibler divergence explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)

**Q**: where the formula of calculating `KL_loss` comes from?
"""

class TPARAM:
    stage = 1
    loss = ['binary_crossentropy']
    optimizer = lambda name : Adam(lr=0.0002, beta_1=0.5, beta_2=0.999) if name == 'Adam' else RMSprop(learning_rate=0.0008, rho=1.0, decay=6e-8) 
    conditioned_text_embedding_dim = 128
    noise_variable_dim = 100
    batch_size = 64
    buffer_size = 10000 # should be greater than or equal to the full size of the dataset
    epochs = 1000
    depth = 512
    img_size = lambda stage : (64, 64) if stage == '1' else (256, 256)
    ds_status = 'TRAIN'
    metrics = ['accuracy']
    ds_path = '/gdrive/My Drive/Birds-Dataset/'


    def Model__save(self, dm, gm):
        if type(dm) is 'Model' and type(gm) is 'Model':
            dm.save('/gdrive/My Drive/GAN-models/stack__GAN/disc')
            gm.save('/gdrive/My Drive/GAN-models/stack__GAN/gen')


    def KL_loss(self, y_true, y_pred): # kl_loss for mean_logsigma distribution of adversarial network output on true text embeddings - operates on the latent tensor
        ''' 
        With KL divergence we can calculate exactly how much information is lost when we approximate one distribution with another.
        Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another (true text embeddings and latent space of them).
        '''
        mean = y_pred[:, :TPARAM.conditioned_text_embedding_dim]
        logsigma = y_pred[:, :TPARAM.conditioned_text_embedding_dim]
        loss = -logsigma + .5 * (-1 + tfx.math.exp(2. * logsigma) + tf.math.square(mean))
        return tf.math.reduce_mean(loss)
         

    def plog__it(self):
        # loading the history pickle files
        with open('/gdrive/My Drive/GAN-models/stack__GAN/genLoss.pickle', 'rb') as fgen:
            genHis = pickle.load(fgen, encoding='latin1')
        with open('/gdrive/My Drive/GAN-models/stack__GAN/discLoss.pickle', 'rb') as fdis:
            discHis = pickle.laod(fdisc, encoding='latin1')
        
        # https://plotly.com/~jefflai108/3/#/
        # https://plotly.com/~jefflai108/5/#/
        
        # dict_of_fig = dict({
        #     "data": [{"type": "bar",
        #             "x": [1, 2, 3],
        #             "y": [1, 3, 2]}],
        #     "layout": {"title": {"text": "Generator Loss"}}
        # })

        # fig = go.Figure(dict_of_fig)
        # fig.show()

"""# **Image Processing Kit**"""

def resize_img(img_path, bbox, img_size):
    width, height = img_size
    img = Image.open(img_path).convert('RGB')

    # Ex - bbox : 60.0 27.0 325.0 304.0 
    # R : 325
    # center_x = 282
    # center_y = 179
    # y1 = 0
    # y2 = 504
    # x1 = 0
    # x2 = 607

    if bbox is not None: # bbox cropping algorithm - loads the image and crops it around the provided bounding box 
        R = int(np.maximum(bbox[2], bbox[3]) * 0.75) # bbox[2] : width, bbox[3] : height
        center_x = int((2*bbox[0] + bbox[2]) / 2) # bbox[0] : x
        center_y = int((2*bbox[1] + bbox[3]) / 2) # bbox[1] : y
        y1 = np.maximum(0, center_y - R)
        y2 = np.maximum(height, center_y + R)
        x1 = np.maximum(0, center_x - R)
        x2 = np.maximum(width, center_x + R)
        img = img.crop([x1, y1, x2, y2]) # crop based on left, upper, right, and lower pixel coordinate
        
    img = img.resize(img_size, Image.BILINEAR)
    return img

def RGB_IMG_save(noise, epoch, nth_img):
    if not os.path.isdir('generated'): os.mkdir('generated')
    fig = px.imshow(noise * 127.5 + 127.5) # turin generated noise pixel into range 0 to 255 
    fig.update_layout(title=f"{epoch}-{nth_img}", width=400, height=400)
    fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)
    fig.write_image(f"generated/sgan_{epoch}-{nth_img}.png")


def Make_Gif(self):
    filenames = [fname for fname in np.sort(os.listdir('generated')) if ".png" in fname]
    with imageio.get_writer('generated/sgan.gif', mode="I") as writer: # open a writer object for writing images on it to export a gif
        for filename in filenames: # for every file in filenames list read them
            image = imageio.imread('generated/'+filename)
            writer.append_data(image) # append opened image into writer object for making gif

"""# **Preparing CUB-Brids Dataset**

**NOTE:** *Remember to merge folders after extracting each one and rename it to `Birds-Dataset` then run the following code cell.*

[CUB-200-2011](https://drive.google.com/open?id=0B3y_msrWZaXLT1BZdVdycDY5TEE)

[images](http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz)
"""

def load_class_ids(path, type):
	with open(path, 'rb') as f:
		classIDs = pickle.load(f, encoding='latin1') # encoding='latin1' is required for unpickling numPy arr
	np.save(f'{TPARAM.ds_path}classes_id-{type}.npy', classIDs) # 8855/2933 trianing/testing image class labels - ids


def load_embeds(path, type):
	with open(path, 'rb') as f:
		embeds = pickle.load(f, encoding='latin1')
	np.save(f'{TPARAM.ds_path}embeddings-{type}.npy', embeds) # 8855/2933 trianing/testing embeddings with 10 words padded in each sample and 1024 length of each word


def load_fnames(path, type):
	with open(path, 'rb') as f:
		fnames = pickle.load(f, encoding='latin1')
	np.save(f'{TPARAM.ds_path}file_names-{type}.npy', fnames) # 8855/2933 trianing/testing file with their names


def load_bbox(bbox_path, file_path):
	df_bounding_boxes = pd.read_csv(bbox_path, delim_whitespace=True, header=None).astype(int)
	df_filenames = pd.read_csv(file_path, delim_whitespace=True, header=None)
	
	img_filenames = df_filenames[1].tolist() # append all image file names into a list - df_filenames[0] => ids | df_filenames[1] => filenames - 11788 images in total
	filename_bbox_dict = {img_file[:-4] : [] for img_file in img_filenames[:2]} # we're removing the jpg extension by :-4 syntax - we fill dictionary with only two first images cause we can't initialize an empty dict at the first step
	
	for i in range(0, len(img_filenames)):
		bbox = df_bounding_boxes.iloc[i][1:].tolist() # we don't want the image id so we start from iloc[i][1:] then we append all those params into a list
		key = img_filenames[i][:-4] # get the ith image name without the extension
		filename_bbox_dict[key] = bbox # fill the above dictionary ----- key : filename , value : bbox - bounding boxes are used to extract objects from the raw images
	
	fn_bbox_dict = open(f"{TPARAM.ds_path}file_name_bounding_boxes.pickle","wb") # saving the fname_bbox dict into a pickle file 
	pickle.dump(filename_bbox_dict, fn_bbox_dict)
	fn_bbox_dict.close()
 

load_class_ids(f'{TPARAM.ds_path}train/class_info.pickle', 'TRAIN')
load_class_ids(f'{TPARAM.ds_path}test/class_info.pickle', 'TEST')

load_embeds(f'{TPARAM.ds_path}train/char-CNN-RNN-embeddings.pickle', 'TRAIN')
load_embeds(f'{TPARAM.ds_path}test/char-CNN-RNN-embeddings.pickle', 'TEST')

load_fnames(f'{TPARAM.ds_path}train/filenames.pickle', 'TRAIN')
load_fnames(f'{TPARAM.ds_path}test/filenames.pickle', 'TEST')

load_bbox(f'{TPARAM.ds_path}bounding_boxes.txt', f'{TPARAM.ds_path}images.txt')

"""# **Building Dataset**

**NOTE:** *First of all create a folder called `final_data` then run following code cell where you have the `images` folder, to build the required `npy` files from `images`, `classes_ids` and `embeddings` tensors for training and testing data.*
"""

def build_ds():
    file_names = np.load(f'{TPARAM.ds_path}file_names-{TPARAM.ds_status}.npy') # numpy arr
    classes_id = np.load(f'{TPARAM.ds_path}classes_id-{TPARAM.ds_status}.npy') # numpy arr
    all_embeddings = np.load(f'{TPARAM.ds_path}embeddings-{TPARAM.ds_status}.npy') # numpy training shape : (8855, 10, 1024) - we have the word2vec tensor!

    with open('file_name_bounding_boxes.pickle', 'rb') as f:
        file_name_bounding_boxes = pickle.load(f, encoding='latin1')

    images_stage1, images_stage2, class_ids, embeddings = [], [], [], []

    for index, fname in tqdm(enumerate(file_names)): # 8855 training images - in every iteration we pick a random embedding vector of size 1024 from 10 words of each sample, so our embeddings shape will be (8855, 1024)
        bbox = file_name_bounding_boxes[fname]

        try:
            img_full_path = f'{TPARAM.ds_path}images/{fname}.jpg' # fname : images/<directory/species_name>/<image_name>
            image_stage1 = resize_img(img_full_path, bbox, TPARAM.img_size(stage='1'))
            image_stage2 = resize_img(img_full_path, bbox, TPARAM.img_size(stage='2'))
            
            # we do this because we want to concatenate text embeddings with generated image for discriminating and we can't the concatenate tensors with size (8855, 10, 1024) and (8855, 64, 64, 3)
            # we can concatenate the latent space of text embeddings tensor with size (None, 4, 4, 128) generated from compress_embeddings function and the latent space of generated image using generator model with size (None, 4, 4, 512)
            embed_idx = np.random.randint(0, all_embeddings[index, :, :].shape[0] - 1) # select a random index between 0 and the number of rows (words) in each sample
            embeddings.append(all_embeddings[index, embed_idx, :]) # our vocab training shape : (8855, 1024) - append a random embedding vector from all embeddings of indexTh sample to the embedding list with size 1024
            images_stage1.append(np.array(image_stage1))
            images_stage2.append(np.array(image_stage1))
            class_ids.append(classes_id[index]) # append the class id for indexTh image to the class_ids list 

        except Exception as e:
            print(f'[An Error is going to kill us! ::: {e}]')

    images_stage1 = np.save(f'{TPARAM.ds_path}final_data/images_stage1-{TPARAM.ds_status}.npy', np.array(images_stage1))
    images_stage2 = np.save(f'{TPARAM.ds_path}final_data/images_stage2-{TPARAM.ds_status}.npy', np.array(images_stage2))
    class_ids = np.save(f'{TPARAM.ds_path}final_data/classes_id-{TPARAM.ds_status}.npy', np.array(class_ids))
    embeddings = np.save(f'{TPARAM.ds_path}final_data/embeddings-{TPARAM.ds_status}.npy', np.array(embeddings))


build_ds()
TPARAM.ds_status = 'TEST'
build_ds()

"""# **$** 
**Loading Dataset Through Tensorflow Input Pipeline**

[meaning of buffer size in dataset](https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle)
"""

images_stage1 = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/images_stage1-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing images for stage1 
images_stage2 = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/images_stage2-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing images for stage2
embeddings = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/embeddings-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing pre-trained text embeddings of size 1024
classes_id = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/classes_id-{TPARAM.ds_status}.npy'))) # 8855/2933 training/testing classes id

if TPARAM.ds_status == 'TRAIN': # create a buffer of at most TPARAM.buffer_size elements, and a background thread to fill that buffer in the background.
    images_stage1 = images_stage1.cache() # allows to cache elements of the dataset for future reusing. Cached data will be store in memory (by default)
    images_stage1 = images_stage1.shuffle(TPARAM.buffer_size, reshuffle_each_iteration=True) # every time when data was needed, it takes from the buffer. After that buffer is filled up with newest elements to the given buffer size and also shuffle order should be different for each epoch with the last flag argument. 
    images_stage1 = images_stage1.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE) # num_parallel_calls should be equal the number of processes that can be used for transformation. tf.data.experimental.AUTOTUNE defines appropriate number of processes that are free for working - preprocessing level ... scale to [-1, 1]
    images_stage1 = images_stage1.batch(TPARAM.batch_size, drop_remainder=True) # drop the last batch cause it doesn't fit the batch size - 8855 images devided into 8855/HPARAM.batch_size batches each of size HPARAM.batch_size
    images_stage1 = images_stage1.prefetch(tf.data.experimental.AUTOTUNE) # defines appropriate number of batches to feed into the next iteration and the maximum number of elements that will be buffered when prefetching - prevent CPU stands idle and allows later elements to be prepared while the current element is being processed.
    images_stage1 = list(images_stage1.as_numpy_iterator()) # shuffled , preprocessed and batched

    images_stage2 = images_stage2.cache() 
    images_stage2 = images_stage2.shuffle(TPARAM.buffer_size, reshuffle_each_iteration=True)
    images_stage2 = images_stage2.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    images_stage2 = images_stage2.batch(TPARAM.batch_size, drop_remainder=True)
    images_stage2 = images_stage2.prefetch(tf.data.experimental.AUTOTUNE)
    images_stage2 = list(images_stage2.as_numpy_iterator()) # shuffled , preprocessed and batched

    embeddings = embeddings.cache()
    embeddings = embeddings.shuffle(TPARAM.buffer_size, reshuffle_each_iteration=True)
    embeddings = embeddings.batch(TPARAM.batch_size, drop_remainder=True)
    embeddings = embeddings.prefetch(tf.data.experimental.AUTOTUNE)
    embeddings = list(embeddings.as_numpy_iterator()) # shuffled and batched

if TPARAM.ds_status == 'TEST':
    images_stage1 = images_stage1.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    images_stage1 = list(images_stage1.as_numpy_iterator())

    images_stage2 = images_stage2.map(lambda x : (tf.cast(x, tf.float32) - 127.5)/127.5, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    images_stage2 = list(images_stage2.as_numpy_iterator())

    embeddings = list(embeddings.as_numpy_iterator())

"""# **Conditional Augmentation Kit**

**Q**: where the formula of calculating `c` comes from?
"""

def conditioned(x): # generate conditional augmentation on text embeddings - x shape : (None, 256)
    '''
    x is the output of a feeded text embeddings into a fully connected dense layer to calculate the mean (μ(φt)) and standard deviation (σ(φt)) by splitting it into two tensors from mean_logsigma output.
    N(μ(φt), Σ(φt)) : gaussian distribution of standard deviations (diagonal of covariance matrix = Σ(φt)) and mean of embedded texts - tf.random.normal(observations, mean, standard_deviation)
    we have to calculate the value of c using above distribution and the formula : mean + standard deviation * epsilon (N(0, I)) which is a normal distribution.
    both μ(φt) and Σ(φt) are learned jointly with the rest of the network in generator model during back propagation.
    =========================================
    logvar = log_sigma
        it makes sense that:
    logvar.exp() = sigma
        Mathematically it is all equivalent.
    '''
    mean = x[:, :TPARAM.conditioned_text_embedding_dim] # μ(φt) - the mean of our feeded text embeddings - output shape : all rows and the first 128 columns
    log_sigma = x[:, TPARAM.conditioned_text_embedding_dim:] # σ(φt) - the diagonal covariance matrix of our feeded text embeddings - output shape : all rows and the second 128 columns 
    standard_deviation = K.exp(log_sigma) # calculate the diagonal of covariance matrix = Σ(φt) which is the standard deviations (σ) - to calculate the sigma or standard deviation we have to pass its log through an exp function
    epsilon = K.random_normal(shape=K.constant((mean.shape[1], ), dtype='int32')) # generate an epsilon normal distribution between 0 to 128
    return standard_deviation * epsilon + mean # we use this variable for both stages with size (None, 128)

def conditional_augmentation_model(): # for generator second output
    '''
    In the this network, the text embedding vector is passed through a fully connected layer with nonlinearity, which produces the mean μ(φt) and the diagonal covariance matrix = Σ(φt).
    the diagonal elements of the covariance matrix = Σ(φt) are the variances and the square root of these variances are the standard deviations (σ).
    '''
    our_input = Input(shape=(1024,)) # (None, 1024)
    mean_logsigma = Dense(256)(our_input) # (None, 256)
    mean_logsigma = LeakyReLU(0.2)(mean_logsigma) # mean_logsigma - predict the mean and standard deviation (the square root of diagonal covariance matrix) - output of a dense layer is mu and logsigma
    # c = Lambda(conditioned)(mean_logsigma)
    return Model(our_input, mean_logsigma) # (None, 1024) -> (None, 256)

"""# **Compressing Text Embeddings**"""

def compress_embeddings__(): # for stage1 discriminator second input
    '''
    compress the embedding samples to a latent space with shape 128.
    giving a tensor of size 1024 and returning a latent space of that tensor with size 128.
    '''
    embeddings_input = Input(shape=1024,)
    output = Dense(TPARAM.conditioned_text_embedding_dim)(embeddings_input)
    output = ReLU()(output)
    return Model(embeddings_input, output)

"""# **Generating 64 x 64 Images using a Random Noise Variable**

![gen architecture](https://drive.google.com/uc?id=1NctCmLFct6LgT9jsT9R4Mz7TDm5MjgXA)
"""

def generator():
    '''
    conditioned on the text embeddings with 1024 features, It takes a random noise vector sampled from a latent space and generates an image with a shape of 64x64x3.
    we use 128 * 8 depth and a 4 x 4 matrix of image size for the starter layers to reach the 64 x 64 image size and a half of the 128 * 8 depth in each iteration.  
    '''
    print("\n\n===================== GENERATOR SUMMARY =====================\n\n")
    first_input = Input(shape=(1024,)) # text embeddings with size (None, 1024) - for training the size is (8855, 1024)
    mean_logsigma = Dense(256)(first_input) # latent space of text embeddings with size (None, 256)
    mean_logsigma = LeakyReLU()(mean_logsigma) # mean_logsigma (μ(φt) and Σ(φt)) are learned jointly during back propagation using KL_loss function to update the text conditioned variable
    c = Lambda(conditioned)(mean_logsigma) # the latent space (output) size : (None, 128) - conditioned on text embeddings
    
    second_input = Input(shape=(TPARAM.noise_variable_dim,)) # random noise varibale with size (None, 100)
    merged_inputs = Concatenate(axis=1)([c, second_input]) # output size : (None, 228) = (None, 100) + (None, 128) - concatenating the generated condition on text and the generated noise (None, 100) from a normal distribution along the columns axis
    
    TPARAM.depth = 512
    output = Dense(int(TPARAM.depth/4) * 8 * 4 * 4, use_bias=False)(merged_inputs) # output size : (None, 16384) - we use 16384 columns to reshape it simply
    output = ReLU()(output)
    output = Reshape((4, 4, int(TPARAM.depth/4) * 8))(output) # reshape the concatenated value to 4, 4, 1024 to double the size till we reach the image size - output size : (None, 4, 4, 1024)
    
    for i in range(4): # we need 4 Conv2D layer to half the filter size from 1024 and double the image size in each iteration
        output = UpSampling2D((2,2))(output)
        output = Conv2D(TPARAM.depth, kernel_size=(3,3), padding="same", strides=1, use_bias=False)(output)
        output = BatchNormalization()(output)
        output = ReLU()(output)
        TPARAM.depth = int(TPARAM.depth / 2)
    
    output = Conv2D(3, kernel_size=(3,3), padding="same", strides=1, use_bias=False)(output) # output size : (None, 64, 64, 3) - generated image
    output = Activation(activation='tanh')(output) # output is [-1, 1]

    TPARAM.depth = int(512/8)
    generator_model = Model([first_input, second_input], [output, mean_logsigma]) # with two inputs we'll have two tensors as our total output - the mean_logsigma output is just a tensor that help us to know how much changes should apply to our text condition variable during back propagation in generator and sgan model.  
    # plot_model(generator_model, to_file='generator_model.png')
    # generator_model.summary()
    return generator_model # return a model with two inputs : noise and the conditioned text embedding & two outputs : the generated image and a latent space of compressed text embeddings with size 256 

# generator()

"""# **Discriminating Generated Images Combined with the Latent Space of Text Embeddings**

![disc architecture](https://drive.google.com/uc?id=1KBh0cX85D2EJ5VNYuxxZytasjtKguzns)
"""

def discriminator():
    '''
    this network contains a set of downsampling layers and classifies whether a given image is real or fake.
    this model takes two inputs : the generated image of size 64 x 64 and the spatial replicated of compressed text embedding with size 128.
    the second input is the half of the second output of generator network, the latent space of compressd text embeddings with size 128 (256 / 2) 
    '''
    print("\n\n===================== DISCRIMINATOR SUMMARY =====================\n\n")
    first_input = Input(shape=(64, 64, 3)) # the generated image
    second_input = Input(shape=(4, 4, TPARAM.conditioned_text_embedding_dim)) # latent space of text embeddings as the second input for discriminator - we use this size that the latent space of text embeddings be concatenable with image size : (None, 4, 4, 128) + (None, 4, 4, 512) = (None, 4, 4, 640)
    
    output = Conv2D(TPARAM.depth, kernel_size=(4,4), padding="same", strides=2, use_bias=False)(first_input) # output size : (None, 32, 32, 64)
    output = LeakyReLU(0.2)(output)

    for i in range(3): # 3 iterations to half the image size and double the filter size to reach the 512 for concatenating with latent space of text embeddings with size 128 for a 640 (128 + 512) as a concatenating size and 4 for latent space size
        TPARAM.depth *= 2
        output = Conv2D(TPARAM.depth, kernel_size=(4, 4), padding='same', strides=2, use_bias=False)(output)
        output = BatchNormalization()(output)
        output = LeakyReLU(0.2)(output)
    
    TPARAM.depth = int(512/8)
    
    merged_inputs = concatenate([output, second_input]) # output size : (None, 4, 4, 128) + (None, 4, 4, 512) = (None, 4, 4, 640) - merging the output of discriminating the generated image and the latent space of text embeddings - compression and spatial replication of the image and text embeddings
    second_output = Conv2D(TPARAM.depth * 8, kernel_size=(1,1), padding="same", strides=1)(merged_inputs) # passing the merged_inputs through a convolutional layer to jointly learn the features across the image and the text
    second_output = BatchNormalization()(second_output)
    second_output = LeakyReLU()(second_output)
    second_output = Flatten()(second_output) # flatten the output of the previous layer
    second_output = Dense(1)(second_output) # output size : (None, 1)
    second_output = Activation('sigmoid')(second_output) # output is [0, 1]
    
    discriminator_model = Model([first_input, second_input], [second_output])
    # plot_model(discriminator_model, to_file='discriminator_model.png')
    # discriminator_model.summary()
    return discriminator_model # return a model with two inputs : the generated image and the latent space of the text embeddings with one output : the discriminated text and the generated image

# discriminator()

"""# **Building Aversarial Network**

[what those values in "Connected to" column of model summary mean?](https://stackoverflow.com/questions/53942291/what-does-the-00-of-the-layers-connected-to-in-keras-model-summary-mean)

![sgan network](https://drive.google.com/uc?id=1iaY04XMFd_94upioRo7afgsSfyiLrju-)
"""

def adversarial_network(gen, disc):
    print("\n\n===================== STACKGAN SUMMARY =====================\n\n")
    embedding_input_for_generator = Input(shape=(1024,)) # training text embeddings
    noise_input_for_generator = Input(shape=(TPARAM.noise_variable_dim,)) # random noise variable
    latent_space_of_text_embeddings = Input(shape=(4, 4, TPARAM.conditioned_text_embedding_dim)) # latent space of text embeddings to merge with down sampled generated image with size (None, 4, 4, 512) for discriminating

    generated_image, mean_logsigma = gen([embedding_input_for_generator, noise_input_for_generator]) # two inputs : training text embeddings and random noise variable - two outputs : generated image and latent space of text embeddings with size (None, 256)
    disc.trainable = False # freez all discriminator weights when we're training the generator, we should do this before compiling the model
    valid_output = disc([generated_image, latent_space_of_text_embeddings]) # two inputs : the generated image and the latent space of text embeddings with size (None, 4, 4, 128) to discriminate the image in a combined normal distribution with latent space of text embeddings


    # texts, noise varaible tensors in a combined normal distribution & latent space of text embeddings -> sgan_model -> generated image validation & latent space of text embeddings with size (None, 256)
    # for our adversarial model in stage1 architecture we should have two outputs : the discriminated images-texts and the latent space of text embeddings with size (None, 256)
    # the generator will maximize the loss of the discriminator on discriminating its output through the discriminator in sgan model by minimizing the adversarial loss so our gen loss is the adversarial loss cause we don't have labels in generator model itself.
    # our second output mean_logsigma is a variable that we calculate the text condition based on; and because of that we have to put it in our sgan output to know how much we have to change the text condition variable respect to mean_logsigma latent space during back propagation of generator network.
    sgan_model = Model([embedding_input_for_generator, noise_input_for_generator, latent_space_of_text_embeddings], [valid_output, mean_logsigma])
    
    # sgan_model.summary()
    # plot_model(sgan_model, to_file='sgan_model.png')
    return sgan_model


# adversarial_network(generator(), discriminator())

"""# **Building, Compiling & Training Process**"""

'''
optimizers are algorithms or methods used to change the attributes of 
your neural network such as weights and learning rate in order to reduce the losses.
optimization algorithms or strategies are responsible for reducing the 
losses and to provide the most accurate results possible
'''

with strategy.scope():
    ca_model = conditional_augmentation_model()
    ca_model.compile(loss=TPARAM.loss, optimizer=TPARAM.optimizer("Adam")) # compiling the conditional augmentation model which its output size is (None, 256)

    gen_model = generator()
    gen_model.summary()
    gen_model.compile(loss="mse", optimizer=TPARAM.optimizer("Adam")) # compiling the generator model to set the loss and optimizer parameters - mean_sqaured_error is a loss function for generated_image and original_image

    disc_model = discriminator()
    disc_model.summary()
    disc_model.compile(loss=TPARAM.loss, optimizer=TPARAM.optimizer("Adam")) # binary_crossentropy is a classification loss function for 0 and 1 class

    ce_model = compress_embeddings__()
    ce_model.compile(loss=TPARAM.loss, optimizer=TPARAM.optimizer("Adam")) # compiling the compress embeddings model which its out put size is (None, 128)


    '''
    optional list or dictionary representing scalar coefficients (Python floating point numbers) for weighting the loss contributions of different model outputs.
    The loss value minimized by the model is loss_weights a weighted sum of individual losses weighted by a coefficient.
    loss_weights parameter on compile is used to define how much each of your model output loss contributes to the final loss value.
    it weighs the model output losses. You could have a model with 2 outputs where one is the primary output and the other auxiliary.
    
    the first output contributes 1 time to the final loss (1 * final loss for first output loss) and the second one which is the mean_logsigma contributes 2 times 
    to the final (2 * final loss for second output loss) loss which has more contribution than the first one.
    total loss = (1 * final loss for first output loss) + (2 * final loss for second output loss)
    
    in order to train the generator we have to train the sgan model on batches cause we don't have labels in generator model and we are comparing 
    the discriminators decisions on the generated images and the generator tries to maximize the discriminator's output for its fake instances.
    
    compiling adversarial model : text embeddings & noise -> gen -> generated noise & latent space of text embeddings -> disc -> valid & mean_logsigma
    we used KL_loss for the second output mean_logsigma of our sgan model
    '''
    sgan_model = adversarial_network(gen=gen_model, disc=disc_model)
    sgan_model.summary()
    sgan_model.compile(loss=["binary_crossentropy", TPARAM.kl_loss], loss_weights=[1, 2.0], optimizer=TPARAM.optimizer("Adam"), metrics=None)


real_labels = tf.ones((TPARAM.batch_size, 1), dtype='float32') * .9 # creating true smoothed labels for our total batch size for training data
fake_labels = tf.zeros((TPARAM.batch_size, 1), dtype='float32') * .1 # creating fake smoothed labels for our totla batch size for generated noises 

print()

for epoch in range(TPARAM.epochs):
    print(f"__________________________________________________________ Epoch {epoch+1} __________________________________________________________")
    print(f"[INFO] Total number of batches for 8855 training images with {TPARAM.batch_size} samples in each batch : {len(images_stage1)}") # the images_stage1 is total training images separated into batches and the length of images_stage1 input pipeline list is the number of batches. 

    generator_losses, discriminator_losses = [], [] # save gen and disc losses for every epoch
    for batch in range(len(images_stage1[0])): # we're gonna train our models for every batch in our training datasets (images_stage1 & embeddings)
        print(f"[INFO] Batch : {batch+1}/{TPARAM.batch_size}")
        z_noise_var = tf.random.normal((TPARAM.batch_size, TPARAM.noise_variable_dim), 0, 1) # creating a random noise variable from a normal distribution
        img_batch = images_stage1[np.random.randint(0, len(images_stage1))] # select a random batch of images pipline
        img_batch = 0.5 * img_batch + 0.5 # rescale to [0, 1] - because all training images have range [-1, 1] and to feed the batch into the discriminator network we have to scale our data to [0, 1]
        embd_batch = embeddings[np.random.randint(0, len(embeddings))] # select a random batch of embeddings pipline


        # our generator takes two inputs : the embedding batch of texts and random noise variable to generate a text conditioned variable with size (batch, 128)
        # based on the latent space with 256 features of text embeddings to combine it with the random noise variable with size (TPARAM.batch_size, 100); 
        # the mean_logsigma will be use and update in generator during backpropagation (because it's a output of generator and our adversarial network and it'll update with loss functions) 
        # to update the conditioned variable for generating the the latent space of text embeddings with size (batch, 128) for discriminator input.
        generated_noise, mean_logsigma = gen_model.predict([embd_batch, z_noise_var], verbose=1)
        generated_noise = 0.5 * generated_noise + 0.5 # scale the image which is between -1 and 1 to 0 and 1 - because the output of discriminator is [0, 1] and we have to scale our input data for the network
        compress_embeddings = ce_model.predict_on_batch(embd_batch) # output size : (batch, 128) - generate a compressed embeddings of a single batch of texts with 128 features for discriminator input to merge with the generated image to discriminate both of them together.


        # because the default shape of compress_embeddings predicted by its model is (batch, 128) we have to reshape our compress_embeddings 
        # to make its shape suitable for discriminator input cause the discriminator takes a latent space with size (batch, 4, 4, 128)
        # here -1 means the batch size and not the batch * TPARAM.conditioned_text_embedding_dim cause we have TPARAM.conditioned_text_embedding_dim in 4th dim
        # there is one row of each (1, TPARAM.conditioned_text_embedding_dim) and batch size row of the whole
        compress_embeddings = np.reshape(compress_embeddings, (-1, 1, 1, TPARAM.conditioned_text_embedding_dim)) # output size : (batch, 1, 1, TPARAM.conditioned_text_embedding_dim)
        compress_embeddings = np.tile(compress_embeddings, (1, 4, 4, 1)) # output size : (batch, 4, 4, TPARAM.conditioned_text_embedding_dim) - tile the input with (1, 4, 4, 1) repetitions to get the desired size for discriminator input
        disc_model.trainable = True # unfreeze the discriminator weights for training


        # train_on_batch method returns a scalar training loss if the model has no metrics and a list of scalars ([0] is loss & [1] is accuracy) if model has metrics available in its compile method.
        # our discriminator model takes two inputs (batch of real images and compress text embeddings that we just prepared)
        # and the true labels of y to run a single gradient update on the image and compress embeddings batch of data
        # below we have real images and their corresponding text as positive sample pairs with true labels.
        disc_positive_sample_pairs_loss = disc_model.train_on_batch([img_batch, compress_embeddings], real_labels)


        # generated images with their corresponding embeddings and fake labels
        # returns the discriminating loss of a single batch of the fake images and compress embeddings respect to fake labels
        disc_negative_sample_pairs_loss_v1 = disc_model.train_on_batch([generated_noise, compress_embeddings], fake_labels)
        
        
        # real images with mismatched embeddings and fake labels - cut two row (image or slices) from the end of the img_batch and also first two rows (0 and 1 embeddings) 
        # from compress_embeddings to get real images with mismatched embeddings (ruin corresponding embeddings of real images with cutting slices); 
        # finally for the y labels (actual values) we have to cut the first two rows of fake_labels (ruin corresponding labels with size (batch, 1) 
        # of training data which is the real images with mismatched embeddings) and reshape to the number of real images that we just dropped its last two rows of it. 
        disc_negative_sample_pairs_loss_v2 = disc_model.train_on_batch([img_batch[:(TPARAM.batch_size - 2)], compress_embeddings[2:]], np.reshape(fake_labels[2:], (TPARAM.batch_size - 2, 1))) 


        disc_model.trainable = False # freeze the discriminator weights to train the generator        
        Dloss = 0.5 * tf.math.add(disc_positive_sample_pairs_loss, 0.5 * tf.math.add(disc_negative_sample_pairs_loss_v1, disc_negative_sample_pairs_loss_v2)) # the total loss of discriminator : positive samples loss + negative samples loss
        Gloss = sgan_model.train_on_batch([embd_batch, z_noise_var, compress_embeddings], [tf.ones((TPARAM.batch_size, 1)) * 0.9, tf.ones((TPARAM.batch_size, 256)) * 0.9]) # takes three inputs as training data and two smoothed tensors as y labels - because our adversarial network has two outputs we have to pass the true labels (real labels) for our data batch : one for valid and the other for mean_logsigma (latent space with size (batch, 256)) update during pack propagation with KL_loss

        
        # we didn't specify metrics in above models(disc, gen, sgan, ce, ca) and because of that the output of train_on_batch method 
        # on one of these models don't have a list of scalars; one scalar only which is the calculated loss on a single batch of data in each iteration. 
        print(f"[INFO] discriminator loss of positive samples : {disc_positive_sample_pairs_loss}")
        print(f"[INFO] discriminator loss of nagative samples - generated images with their correspond text embeddings : {disc_negative_sample_pairs_loss_v1}")
        print(f"[INFO] discriminator loss of nagative samples - real images with mismatched text embeddings : {disc_negative_sample_pairs_loss_v2}")
        print(f"[INFO] discriminator loss : {Dloss}")
        print(f"[INFO] generator loss - adversarial loss with 3 input and 2 output tensors : {Gloss}")
        print("_______________________________________________________________________________________________________________")

        generator_losses.append({'generator loss': Gloss})
        discriminator_losses.append({'discriminator loss': Dloss})

        with open('/gdrive/My Drive/GAN-models/stack__GAN/genLoss.pickle', 'wb') as fgen:
            pickle.dump(generator_losses, fgen)
        with open('/gdrive/My Drive/GAN-models/stack__GAN/discLoss.pickle', 'wb') as fdisc:
            pickle.dump(discriminator_losses, fdisc)

    if epoch % 4 is 0: # save generated images every 4 epoch
        z_noise_var__ = tf.random.normal((TPARAM.batch_size, TPARAM.noise_variable_dim), 0, 1) # generate noise for every batch with 100 features
        embeddings__ = tf.data.Dataset.from_tensor_slices(np.load(os.path.join(TPARAM.ds_path+f'final_data/embeddings-TEST.npy'))) # 2933 testing pre-trained text embeddings of size 1024
        embeddings__ = list(embeddings__.as_numpy_iterator())
        embeddings_batch = np.asarray(embeddings__[0:TPARAM.batch_size]) # select a batch of test embeddings for testing the generator model
        fake_images, mean_logsigma = gen_model.predict_on_batch([embeddings_batch, z_noise_var__]) # got two outputs - predict a single batch of data with test embeddings and a normal distribution of random noise variable

        for nth_img, image in enumerate(fake_images[:25]): # first 25 images with nth_img as index
            RGB_IMG_save(image, epoch, nth_img) # save first 25 generated images with unseen text data (testing text embeddings)


TPARAM.Model__save(dm=isc_model, gm=gen_model) # save trained models
TPARAM.plog__it(generator_losses, discriminator_losses)
Make_Gif() # make gif of all generated images

"""**StackGAN Model Prediction API for Stage1 - Tensorflow Serving**

**NOTE**: *before testing remember to change the `ds_status` of `TPARAM` class to `TEST` and load the testing data into tensorflow pipleline by running the code of `$` cell*.
"""

# TODO : https://www.tensorflow.org/tfx/tutorials/serving/rest_simple
# TODO : load the models, test it with testing data and build the prediction api
# OUTPUT : high res image from a text


# print(images_stage1)
# print(images_stage2)
# print(embeddings)